{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a0e90f-ea6c-49a6-8f9c-4cc1d89706a1",
   "metadata": {},
   "source": [
    "# 02 - Loading Data\n",
    "TEEHR comes with utilities to fetch and format data from a few different sources and store to the TEEHR data format.  We are open to adding more, but also attempted to make the format simple enough that getting your own data into the TEEHR format should be relatively easy - create a Pandas DataFrame with the correct column names and save a Parquet file.\n",
    "\n",
    "## The included loading utilities are:\n",
    "\n",
    "- NWM v2.1 and NWM v2.2 feature data in Google Cloud\n",
    "- NWM v2.1 and NWM v2.2 forcing data in Google Cloud \n",
    "- NWM v2.0 and NWM v2.1 Retrospective in AWS\n",
    "- USGS NWIS Data\n",
    "\n",
    "This covers both feature data (e.g., data at NWM features) as well as aggregated grid data (e.g., mean areal precipitation for catchments). Caching data can talka a significant amopunt of time so we have attempted to make these tools as efficient as possible.\n",
    "\n",
    "| Data                       | Increment                                      | Time                         |\n",
    "| -------------------------- | ---------------------------------------------- | ---------------------------- |\n",
    "| NWM v2.2 Feature Data      | 1 medium range forcast at ~7600 USGS gages     | X mins                       |\n",
    "| NWM v2.2 Forcing Data      | 1 medium range forcast aggregated to HUC10     | Y mins                       |\n",
    "| NWM v2.1 Retrospective     |\n",
    "| USGS NWIS Data             |\n",
    "\n",
    "The following is an example of of the loading tools would be utilized.  Scroll to the bottom for a list of the datasets that were cached for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a0003-e9be-4f1d-9e83-790ad3148609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import teehr.loading.nwm_point_data as nwmp\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745dba2-b125-429d-a1e6-21c434e0fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some notebook variables to point to the relevant study files.  \n",
    "RUN = \"short_range\"\n",
    "OUTPUT_TYPE = \"channel_rt\"\n",
    "VARIABLE_NAME = \"streamflow\"\n",
    "\n",
    "START_DATE = \"2023-03-18\"\n",
    "INGEST_DAYS = 1\n",
    "\n",
    "OUTPUT_ROOT = Path(Path().home(), \"cache\")\n",
    "JSON_DIR = Path(OUTPUT_ROOT, \"zarr\", RUN)\n",
    "OUTPUT_DIR = Path(OUTPUT_ROOT, \"timeseries\", RUN)\n",
    "\n",
    "# For this simple example, we'll get data for 10 NWM reaches that coincide with USGS gauges\n",
    "LOCATION_IDS = [7086109,  7040481,  7053819,  7111205,  7110249, 14299781, 14251875, 14267476,  7152082, 14828145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60d48e-1847-498d-9909-d004c7693a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5e691-506d-46ee-9f66-e18e5de09a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?nwmp.nwm_to_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a1ec7-a8ca-45cd-869d-a3c7bcc26b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nwmp.nwm_to_parquet(\n",
    "    run: str,\n",
    "    output_type: str,\n",
    "    variable_name: str,\n",
    "    start_date: Union[str, datetime.datetime],\n",
    "    ingest_days: int,\n",
    "    location_ids: Iterable[int],\n",
    "    json_dir: str,\n",
    "    output_parquet_dir: str,\n",
    "    t_minus_hours: Optional[Iterable[int]] = None,\n",
    ")\n",
    "\"\"\"\n",
    "nwmp.nwm_to_parquet(\n",
    "    RUN,\n",
    "    OUTPUT_TYPE,\n",
    "    VARIABLE_NAME,\n",
    "    START_DATE,\n",
    "    INGEST_DAYS,\n",
    "    LOCATION_IDS,\n",
    "    JSON_DIR,\n",
    "    OUTPUT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13aa731-9cb0-4540-a357-1603b52de29b",
   "metadata": {},
   "source": [
    "## Cached Data\n",
    "We cached several datasets on AWI's 2i2c JupyterHub shared drive for this workshop that we will be exploring in susequent sections.\n",
    "Because we are using small JupyterHub instances, we will be somewhat selective about what and how much data we query.\n",
    "\n",
    "Looking at the `tree` output for the cache directory we have setup shows the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc78d9-d43a-4f0a-b1e3-9730c13e1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree ~/shared/rti-eval -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2a764-16cc-4d6a-92b0-628364b9eb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
