{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f28e53-63b8-48e1-8017-0c4c85997cae",
   "metadata": {},
   "source": [
    "# 07 - Interactive Queries\n",
    "\n",
    "The examples in the previous notebooks, we learned how to run TEEHR queries by manually defining all components of the query.  Now that we know how the queries work, we will add interactive Panel widgets and a filter-building wrapper to make it even easier to query the portion of data you are interested in.\n",
    "\n",
    "Note that for Notebooks 07-09, the file `postevent_dashboard_utils.py` must be imported.  It provides additional functionality needed for generating dashboards for the post-event evaluation examples. Some of this functionality may become part of the TEEHR package in the near future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efca43-3791-4e08-a82e-73e0b571ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import postevent_dashboard_utils as du\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import panel as pn\n",
    "import colorcet as cc\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a32d2-6427-4a14-8b73-5384d814dbf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define the set parquet files for each specific study/scenario\n",
    "\n",
    "For this example, we use dictionaries to define multiple sets of parquet files needed to evaluate operational NWM short- and medium-range streamflow and precipitation forecasts.  As described in previous notebooks, these files define the source of verifying data (primary_filepath), data to evaluate (secondary_filepath), as well as the necessary geometry, crosswalk, and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435147c1-0704-497d-aa10-d4a37432dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall study directory\n",
    "CACHE_DIR = Path(Path.home(), \"shared\", \"teehr-workshop\", \"post-event-example\")\n",
    "\n",
    "# medium range streamflow forecast evaluation files \n",
    "MRF_streamflow = dict(\n",
    "    scenario_name=\"medium_range\",\n",
    "    variable=\"streamflow\",\n",
    "    primary_filepath=Path(CACHE_DIR, \"timeseries\", \"usgs\", \"*.parquet\"),\n",
    "    secondary_filepath=Path(CACHE_DIR, \"timeseries\", \"medium_range_mem1\", \"*.parquet\"),\n",
    "    crosswalk_filepath=Path(CACHE_DIR, \"geo\", \"usgs_nwm22_crosswalk.parquet\"),\n",
    "    geometry_filepath=Path(CACHE_DIR, \"geo\", \"usgs_geometry.parquet\")\n",
    ")\n",
    "# medium range precip forecast evaluation files\n",
    "MRF_forcing = dict(\n",
    "    scenario_name=\"medium_range\",\n",
    "    variable=\"precipitation\",    \n",
    "    primary_filepath=Path(CACHE_DIR, \"timeseries\", \"forcing_analysis_assim\", \"*.parquet\"),\n",
    "    secondary_filepath=Path(CACHE_DIR, \"timeseries\", \"forcing_medium_range\", \"*.parquet\"),\n",
    "    crosswalk_filepath=Path(CACHE_DIR, \"geo\", \"huc10_huc10_crosswalk.parquet\"),\n",
    "    geometry_filepath=Path(CACHE_DIR, \"geo\", \"huc10_geometry.parquet\"),\n",
    ")\n",
    "# short range streamflow forecast evaluation files \n",
    "SRF_streamflow = dict(\n",
    "    scenario_name=\"short_range\",\n",
    "    variable=\"streamflow\",\n",
    "    primary_filepath=MRF_streamflow[\"primary_filepath\"],\n",
    "    secondary_filepath=Path(CACHE_DIR, \"timeseries\", \"short_range\", \"*.parquet\"),\n",
    "    crosswalk_filepath=MRF_streamflow[\"crosswalk_filepath\"],\n",
    "    geometry_filepath=MRF_streamflow[\"geometry_filepath\"],\n",
    ")\n",
    "# medium range precip forecast evaluation files\n",
    "SRF_forcing = dict(\n",
    "    scenario_name=\"short_range\",\n",
    "    variable=\"precipitation\",    \n",
    "    primary_filepath=MRF_forcing[\"primary_filepath\"],\n",
    "    secondary_filepath=Path(CACHE_DIR, \"timeseries\", \"forcing_short_range\", \"*.parquet\"),\n",
    "    crosswalk_filepath=MRF_forcing[\"crosswalk_filepath\"],\n",
    "    geometry_filepath=MRF_forcing[\"geometry_filepath\"],\n",
    ")\n",
    "attribute_paths = dict(\n",
    "    usgs_upstream_area=Path(CACHE_DIR, \"geo\", \"usgs_attr_upstream_area.parquet\"),\n",
    "    usgs_ecoregions=Path(CACHE_DIR, \"geo\", \"usgs_attr_ecoregions.parquet\"),\n",
    "    usgs_stream_order=Path(CACHE_DIR, \"geo\", \"usgs_attr_stream_order.parquet\"),\n",
    "    usgs_huc_crosswalk=Path(CACHE_DIR, \"geo\", \"usgs_huc12_crosswalk.parquet\"),\n",
    ")\n",
    "\n",
    "# put the scenarios in a list for widget purposes\n",
    "scenario_definitions = [MRF_streamflow, MRF_forcing, SRF_streamflow, SRF_forcing]\n",
    "\n",
    "## general units ('english' or 'metric') to show in visualization\n",
    "viz_units = \"metric\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbf293-7bd5-416f-abde-ed4633353340",
   "metadata": {},
   "source": [
    "### First select the scenario and variable\n",
    "Set up dropdown menus to select the specific scenario to evaluate based on the scenario name and variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1acd6-2abb-4a90-b22c-af9e0084eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_selector = du.get_scenario_selector(scenario_name_list=sorted(du.get_scenario_names(scenario_definitions)))  \n",
    "variable_selector = du.get_variable_selector(variable_list=du.get_scenario_variables(scenario_definitions))   \n",
    "pn.Row(scenario_selector, variable_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156accfc-8448-4a90-8b5a-b22a68864dcf",
   "metadata": {},
   "source": [
    "### Then select the region, time period, characteristics and metrics of interest\n",
    "\n",
    "Set up dropdowns and sliders to get the list or range of options and enable you to easily select a particular subset of data for metric calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42747978-5ce3-4bc1-86c6-229a3e11e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = du.get_scenario(scenario_definitions, scenario_selector.value, variable_selector.value)\n",
    "widgets = du.get_filter_widgets(scenario=scenario, include_widgets=[\n",
    "    'value_time','reference_time','lead_time','huc2','threshold','stream_order','metrics'])\n",
    "\n",
    "pn.Row(\n",
    "    pn.Column(widgets['huc2'], widgets['stream_order'], widgets['threshold'], widgets['metrics']),\n",
    "    pn.Spacer(width=50),    \n",
    "    pn.Column(     \n",
    "        pn.Spacer(height=10), widgets['value_time'],\n",
    "        pn.Spacer(height=10), widgets['reference_time'],\n",
    "        pn.Spacer(height=5), widgets['lead_time'],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe8b45-a1f9-4aeb-b9ac-1152ea30ec45",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make selections\n",
    "\n",
    "Using the menus and sliders above, make selections to evaluate a particular subset of the data.  Many more filters are possible here, these are just a few examples we anticipate may be commonly needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b3524-a1a6-4015-967b-e9d52afaf956",
   "metadata": {},
   "source": [
    "### Run the query\n",
    "Run the cell below to execute a wrapper which builds the filter portion of the TEEHR query and then executes the query.  Experiment with the filter widget selections to see how the run time and query output change.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e62de-e11c-4fc1-9965-79bb3f151faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdf = du.run_teehr_query(\n",
    "    query_type=\"metrics\",\n",
    "    scenario=scenario,\n",
    "    huc_id=widgets['huc2'].value,\n",
    "    order_limit=widgets['stream_order'].value,\n",
    "    value_time_start=widgets['value_time'][1].value_start,    \n",
    "    value_time_end=widgets['value_time'][1].value_end,    \n",
    "    reference_time_start=widgets['reference_time'][1].value_start,    \n",
    "    reference_time_end=widgets['reference_time'][1].value_end,\n",
    "    group_by=['primary_location_id'],\n",
    "    order_by=['primary_location_id'],\n",
    "    value_min=widgets['threshold'].value,    \n",
    "    include_metrics=widgets['metrics'].value,\n",
    "    attribute_paths=attribute_paths,\n",
    "    return_query=False,\n",
    ")\n",
    "display(gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326dc4b-7948-4f42-a31b-9d44d1230d7d",
   "metadata": {},
   "source": [
    "### Create a simple map of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5d905-36b8-4ddd-9ecf-bed1d72c2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "gdf['relative_peak_error'] = gdf['max_value_delta']/gdf['primary_maximum']\n",
    "gdf.hvplot.points(c='relative_peak_error', cmap=cc.CET_D1A[::-1], clim=(-1,1), width=600, height=400,\n",
    "                             title=\"Relative Peak Error\", size=10, xaxis = None, yaxis = None, tiles='CartoLight', \n",
    "                             hover_cols=['primary_location_id', 'primary_maximum'], cnorm='linear')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec1285-bf27-42e7-ba09-11867a40ff9c",
   "metadata": {},
   "source": [
    "### Choose a gage to explore more closely\n",
    "Using the zoom and hover features on the map, select a gage and manually enter the ID in the cell below. (Gage selection by clicking on points in the map will be demonstrated in notebook 09)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76830e27-e235-4938-b2c5-d66f882e4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_id = 'usgs-11451715'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11f09e-e6ce-4608-8ab8-eed868398d93",
   "metadata": {},
   "source": [
    "Using the wrapper again, build and run a TEEHR timeseries query to extract the streamflow timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f367d-4a0d-4c61-8480-03fb433ea594",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df = du.run_teehr_query(\n",
    "    query_type=\"timeseries\",\n",
    "    scenario=scenario,\n",
    "    location_id=usgs_id,\n",
    "    order_limit=widgets['stream_order'].value,\n",
    "    value_time_start=widgets['value_time'][1].value_start,    \n",
    "    value_time_end=widgets['value_time'][1].value_end,    \n",
    "    reference_time_start=widgets['reference_time'][1].value_start,    \n",
    "    reference_time_end=widgets['reference_time'][1].value_end,\n",
    "    value_min=widgets['threshold'].value,    \n",
    "    attribute_paths=attribute_paths,\n",
    "    return_query=False,\n",
    "    include_geometry=False,\n",
    ")\n",
    "display(flow_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bc9cd-7335-4b64-89d6-ddaa60b7b6a8",
   "metadata": {},
   "source": [
    "### Plot the timeseries by reference time\n",
    "Loop through the time series, sequentially adding an additional curve to the holoviews overlay `hydrographs` and further overlay with the observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcfabfe-dde2-4826-9cb4-09910efacd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_df = flow_df.rename(columns={'primary_value':'obs_flow','secondary_value':'fcst_flow'})\n",
    "ref_times = sorted(flow_df['reference_time'].unique())\n",
    "cmap = cc.rainbow[::-1]\n",
    "cstep = int(np.floor(len(cmap) / len(ref_times)))\n",
    "\n",
    "flow_obs = flow_df.hvplot(x='value_time', y='obs_flow', label = 'observed', ylabel='Flow (cms)', color = 'black', line_width=4)   \n",
    "hydrographs = flow_obs\n",
    "for t, time in enumerate(ref_times):\n",
    "    df_t = flow_df[flow_df['reference_time'] == time]\n",
    "    ci = cmap[cstep * t]\n",
    "    fcst = df_t.hvplot(x='value_time', y='fcst_flow', color = [ci])\n",
    "    hydrographs = hydrographs * fcst\n",
    "\n",
    "hydrographs * flow_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8a732-3b54-4a4a-b023-6350b8c589d2",
   "metadata": {},
   "source": [
    "### Now let's get the precipitation analysis and forecasts as well\n",
    "\n",
    "For basic comparison purposes, find and extract the mean areal precipitation time series for the HUC10 containing the above gage.  Eventually we will want to calculate the effective mean areal timeseries for the entire upstream drainage basin associated with a gage or other point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbf607-9ca2-4a53-b7a8-99fe922d549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the usgs-huc crosswalk, get the HUC10 containing the above gage\n",
    "cross = pd.read_parquet(attribute_paths['usgs_huc_crosswalk'])\n",
    "huc12_id = cross.loc[cross['primary_location_id']==usgs_id, 'secondary_location_id'].iloc[0]\n",
    "huc10_id = \"-\".join(['huc10', huc12_id.split(\"-\")[1][:10]])\n",
    "\n",
    "pcp_df = du.run_teehr_query(\n",
    "    query_type=\"timeseries\",\n",
    "    scenario=MRF_forcing,\n",
    "    location_id=huc10_id,\n",
    "    value_time_start=widgets['value_time'][1].value_start,    \n",
    "    value_time_end=widgets['value_time'][1].value_end,    \n",
    "    reference_time_start=widgets['reference_time'][1].value_start,    \n",
    "    reference_time_end=widgets['reference_time'][1].value_end,\n",
    "    attribute_paths=attribute_paths,\n",
    "    return_query=False,\n",
    "    include_geometry=False,\n",
    ")\n",
    "display(pcp_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67479f8-0c98-420b-9f83-8f2559c00be3",
   "metadata": {},
   "source": [
    "Now let's create linked precipitation and streamflow time series overlay plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12459b46-22d3-4852-bc30-781009b88884",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_df = pcp_df.rename(columns={'primary_value':'obs_pcp','secondary_value':'fcst_pcp'})\n",
    "pcp_obs = pcp_df.hvplot(x='value_time', y='obs_pcp', label='observed', ylabel='Precip (mm)', color = 'black', line_width=3)   \n",
    "hyetograph = pcp_obs\n",
    "for t, time in enumerate(ref_times):\n",
    "    df_t = pcp_df[pcp_df['reference_time'] == time]\n",
    "    ci = cmap[cstep * t]\n",
    "    fcst = df_t.hvplot(x='value_time', y='fcst_pcp', color = [ci])\n",
    "    hyetograph = hyetograph * fcst\n",
    "\n",
    "((hyetograph * pcp_obs) + (hydrographs * flow_obs)).cols(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
